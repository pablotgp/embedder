# -*- coding: utf-8 -*-
"""mainipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DNTN2NtuOrcD-eiQAl31Y937Wpt5vqBJ
"""
# -*- coding: utf-8 -*-
import os
import re
import uuid
import faiss
import fitz  # PyMuPDF
import nltk
import numpy as np
import tempfile
import traceback

from collections import defaultdict
from fastapi import FastAPI, UploadFile, File, HTTPException
from dotenv import load_dotenv
from rank_bm25 import BM25Okapi
from sentence_transformers import CrossEncoder
from langchain_openai import OpenAIEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from pinecone import Pinecone

# ==========================
# CONFIG
# ==========================
load_dotenv()

EMBEDDING_MODEL_NAME = "text-embedding-3-small"
RERANKER_MODEL = "cross-encoder/ms-marco-MiniLM-L-12-v2"
LLM_MODEL_NAME = "gemini-1.5-flash-latest"
DIMENSION = 1536  # text-embedding-3-small

K_FAISS_INITIAL = 100
K_BM25_INITIAL = 100
K_RERANK = 50
USE_DYNAMIC_K = True
RERANKER_SCORE_THRESHOLD = 1.5
MIN_CHUNKS_DYNAMIC = 3
MAX_CHUNKS_DYNAMIC = 7

# Estado global
app_state = {"is_initialized": False}
app = FastAPI()


def clean_pdf_text_robust(text):
    """Limpia texto de PDF de forma MÁS robusta para RAG, atacando patrones específicos."""
    if not text: return ""
    # --- PASOS DE LIMPIEZA GENERAL ---
    ligatures = {'ﬁ': 'fi', 'ﬂ': 'fl', 'ﬀ': 'ff', 'ﬃ': 'ffi', 'ﬄ': 'ffl'}
    for lig, repl in ligatures.items(): text = text.replace(lig, repl)
    text = re.sub(r'(\w)-\s*\n\s*(\w)', r'\1\2', text) # Unir palabras con guión
    text = re.sub(r'(\w)-\s*\n\s*(\w)', r'\1\2', text) # Segunda pasada
    text = re.sub(r'^\s*Página\s+\d+(\s+de\s+\d+)?\s*$', '', text, flags=re.MULTILINE | re.IGNORECASE) # Paginación
    text = re.sub(r'\b\d+\s*/\s*\d+\b', '', text) # Paginación X / Y
    text = re.sub(r'https?://[^\s/$.?#].[^\s]*', '', text, flags=re.IGNORECASE) # URLs http/https
    text = re.sub(r'\bwww\.[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}\b(?!\.)', '', text) # URLs www
    text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '', text) # Emails

    # --- REGLAS ESPECÍFICAS MEJORADAS ---
    text = re.sub(r'https?://opo\.cl/[a-zA-Z0-9]+', '', text, flags=re.IGNORECASE) # URLs opo.cl
    text = re.sub(r'\bopositatest\.com\b', '', text, flags=re.IGNORECASE) # Dominio específico
    text = re.sub(r'\bv\d+\.\d+\.\d+\b', '', text, flags=re.IGNORECASE) # Versión vX.Y.Z
    text = re.sub(r'/?\s*\+34\s*(\d{1,3}\s*){2,4}', '', text) # Teléfono +34
    text = re.sub(r'\b\d+\s+T\.\s+\d+\b', '', text) # Marcador "4 T. 8"
    text = re.sub(r'^\s*T\.\s+\d+\b', '', text, flags=re.MULTILINE) # Marcador "T. 8" al inicio línea
    text = re.sub(r'\s+T\.\s+\d+\b', '', text)     # Marcador " T. 8" en medio
    text = re.sub(r'\bT\.\s+\d+\s*', '', text)     # Marcador "T. 8 " general
    text = re.sub(r'^\s*\d+\s+TEMARIO\s*$', '', text, flags=re.MULTILINE | re.IGNORECASE)
    text = re.sub(r'^\s*BIOLOGÍA\s+Y\s+GEOLOGÍA\s+\d+\s*$', '', text, flags=re.MULTILINE | re.IGNORECASE)
    text = re.sub(r'^\s*MATEMÁTICAS\s+\d+\s*$', '', text, flags=re.MULTILINE | re.IGNORECASE)
    text = re.sub(r'^\s*Accede a los recursos.*$', '', text, flags=re.MULTILINE | re.IGNORECASE) # Línea recursos
    text = re.sub(r'^\s*Comprueba si tu temario.*$', '', text, flags=re.MULTILINE | re.IGNORECASE) # Línea actualizado
    text = re.sub(r'^\s*ORGANIZACIÓN DEL ESTADO\s*\|\s*TEMA\s*\d+\s*$', '', text, flags=re.MULTILINE | re.IGNORECASE) # Cabecera específica
    text = re.sub(r'^\s*RECURSOS\s*\n?\s*(GRÁFICOS)?\s*$', '', text, flags=re.MULTILINE | re.IGNORECASE) # Cabecera Recursos
    text = re.sub(r'^\s*\d+\s*$', '', text, flags=re.MULTILINE) # Líneas solo con número (experimental)
    # Eliminar bloque explicativo iconos (más agresivo)
    text = re.sub(r'^\s*RECURSOS\s+GRÁFICOS.*?simple vistazo\.', '', text, flags=re.IGNORECASE | re.DOTALL | re.MULTILINE)
    text = re.sub(r'^\s*PLAZOS\s+Sabemos que.*?simple vistazo\.', '', text, flags=re.IGNORECASE | re.DOTALL | re.MULTILINE)
    text = re.sub(r'^\s*(PLAZOS|Destacados|Pregunta de examen|Datos importantes|Negrita)\s*$', '', text, flags=re.MULTILINE | re.IGNORECASE) # Títulos sueltos iconos

    # --- PASOS DE NORMALIZACIÓN FINAL ---
    text = re.sub(r'[ \t\f\v]+', ' ', text) # Normalizar espacios horizontales
    text = re.sub(r' +\n', '\n', text) # Espacios antes de salto
    text = re.sub(r'\n +', '\n', text) # Espacios después de salto
    text = re.sub(r'\n{3,}', '\n\n', text) # Reducir saltos múltiples a 2
    text = re.sub(r'^\s*\n', '', text, flags=re.MULTILINE) # Eliminar líneas vacías residuales
    text = re.sub(r'^\s*[-•*o»·]\s+', '- ', text, flags=re.MULTILINE) # Normalizar viñetas
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text) # Caracteres de control
    text = text.strip() # Limpiar inicio/fin
    if text: text = text.rstrip('\n') + '\n\n' # Asegurar que termine con dos saltos
    return text

# --- DETECCIÓN PORTADA ---
def is_likely_cover(page_text, page_number, num_total_pages):
    """Heurística para detectar portadas."""
    lines = [line for line in page_text.split('\n') if line.strip()]
    line_count = len(lines)
    text_length = len(page_text.strip())
    if page_number == 0 and (line_count < 15 or text_length < 200):
         return True
    if page_number < 2 and line_count < 25:
        if re.search(r'\b(temario|edición|editorial|reservados todos los derechos|oposici[oó]n)\b', page_text, re.IGNORECASE):
            return True
    return False

# --- DETECCIÓN ÍNDICE/TOC ---
# --- Celda 2 (MODIFICADA) ---
# ... (resto de importaciones y funciones) ...

# --- Celda 2 (MODIFICADA con is_likely_toc_or_index MUY CONSERVADORA) ---
# ... (resto de importaciones y funciones: detect_formulas, is_likely_cover) ...
def detectar_secciones_matematicas(texto_chunk):
    """
    Detecta menciones a secciones matemáticas típicas:
      - Teorema / Theorem
      - Definición / Definition
      - Demostración / Prueba / Proof
      - Lema / Lemma
      - Proposición / Proposition
      - Corolario / Corollary
      - Observación / Observation / Remark
      - Nota / Note

    Retorna un dict con listas de coincidencias y booleanos indicando si se halló algo.
    """

    if not texto_chunk:
        return {
            "teoremas_mencionados": None,
            "definiciones_mencionadas": None,
            "demostraciones_mencionadas": None,
            "lemmas_mencionadas": None,
            "proposiciones_mencionadas": None,
            "corolarios_mencionados": None,
            "observaciones_mencionadas": None,
            "notas_mencionadas": None,

            "is_teorema": False,
            "is_definicion": False,
            "is_demostracion": False,
            "is_lemma": False,
            "is_proposicion": False,
            "is_corolario": False,
            "is_observacion": False,
            "is_nota": False
        }

    # Normalizar espacios y, si deseas, usar re.IGNORECASE
    texto_normalizado = re.sub(r'\s+', ' ', texto_chunk).strip()

    # Patrones. Cada uno busca la palabra clave (en español o inglés) y opcionalmente un número tipo "3.1".
    # Ajusta según el idioma que uses más frecuentemente.
    teorema_pattern = r'\b(T[eé]orema(\s+\d+(\.\d+)*)?|(T|t)heorem(\s+\d+(\.\d+)*)?)\b'
    definicion_pattern = r'\b(Definici[oó]n(\s+\d+(\.\d+)*)?|(D|d)efinition(\s+\d+(\.\d+)*)?)\b'
    demostracion_pattern = r'\b(Demostraci[oó]n(\s+\d+(\.\d+)*)?|(P|p)rueba(\s+\d+(\.\d+)*)?|(P|p)roof(\s+\d+(\.\d+)*)?)\b'
    lema_pattern = r'\b(Lema(\s+\d+(\.\d+)*)?|(L|l)emma(\s+\d+(\.\d+)*)?)\b'
    proposicion_pattern = r'\b(Proposici[oó]n(\s+\d+(\.\d+)*)?|(P|p)roposition(\s+\d+(\.\d+)*)?)\b'
    corolario_pattern = r'\b(Corolario(\s+\d+(\.\d+)*)?|(C|c)orollary(\s+\d+(\.\d+)*)?)\b'
    observacion_pattern = r'\b(Observaci[oó]n(\s+\d+(\.\d+)*)?|(O|o)bservation(\s+\d+(\.\d+)*)?|(R|r)emark(\s+\d+(\.\d+)*)?)\b'
    nota_pattern = r'\b(Nota(\s+\d+(\.\d+)*)?|(N|n)ote(\s+\d+(\.\d+)*)?)\b'

    # Extrae coincidencias. Con re.findall(..., flags=re.IGNORECASE), no necesitarías duplicar paréntesis de mayúsc/minúsc,
    # pero aquí lo dejamos para ser explícitos con la parte en español vs inglés.
    teoremas = re.findall(teorema_pattern, texto_normalizado, flags=re.IGNORECASE)
    definiciones = re.findall(definicion_pattern, texto_normalizado, flags=re.IGNORECASE)
    demostraciones = re.findall(demostracion_pattern, texto_normalizado, flags=re.IGNORECASE)
    lemmas = re.findall(lema_pattern, texto_normalizado, flags=re.IGNORECASE)
    proposiciones = re.findall(proposicion_pattern, texto_normalizado, flags=re.IGNORECASE)
    corolarios = re.findall(corolario_pattern, texto_normalizado, flags=re.IGNORECASE)
    observaciones = re.findall(observacion_pattern, texto_normalizado, flags=re.IGNORECASE)
    notas = re.findall(nota_pattern, texto_normalizado, flags=re.IGNORECASE)

    # Cada lista (p.ej. teoremas) contiene tuplas por los subgrupos capturados.
    # Nos interesa la posición [0] que es el match principal:
    def extraer_matches(l_matches):
        # Convertimos a set para quitar duplicados, luego a list, sorted
        if not l_matches:
            return None
        return sorted(list(set(m[0] for m in l_matches)))

    teoremas_limpios = extraer_matches(teoremas)
    definiciones_limpias = extraer_matches(definiciones)
    demostraciones_limpios = extraer_matches(demostraciones)
    lemmas_limpios = extraer_matches(lemmas)
    proposiciones_limpios = extraer_matches(proposiciones)
    corolarios_limpios = extraer_matches(corolarios)
    observaciones_limpios = extraer_matches(observaciones)
    notas_limpios = extraer_matches(notas)

    # Retornamos un diccionario con las listas y con flags booleanos
    return {
        "teoremas_mencionados": teoremas_limpios,
        "definiciones_mencionadas": definiciones_limpias,
        "demostraciones_mencionadas": demostraciones_limpios,
        "lemmas_mencionadas": lemmas_limpios,
        "proposiciones_mencionadas": proposiciones_limpios,
        "corolarios_mencionados": corolarios_limpios,
        "observaciones_mencionadas": observaciones_limpios,
        "notas_mencionadas": notas_limpios,

        "is_teorema": bool(teoremas_limpios),
        "is_definicion": bool(definiciones_limpias),
        "is_demostracion": bool(demostraciones_limpios),
        "is_lemma": bool(lemmas_limpios),
        "is_proposicion": bool(proposiciones_limpios),
        "is_corolario": bool(corolarios_limpios),
        "is_observacion": bool(observaciones_limpios),
        "is_nota": bool(notas_limpios)
    }

def detectar_paginas_resumen_biblio(pdf_path, max_paginas_finales_a_revisar=10):
    """
    Detecta páginas que contienen RESUMEN o BIBLIOGRAFÍA por separado.
    """
    paginas_resumen = []
    paginas_biblio = []

    resumen_keywords = ['RESUMEN', 'CONCLUSIÓ']
    biblio_keywords = ['BIBLIOGRAFÍA', 'REFERENCIAS', 'WEBGRAFÍA']

    try:
        doc = fitz.open(pdf_path)
        num_total_pages = len(doc)
        start_page_index = max(0, num_total_pages - max_paginas_finales_a_revisar)

        for page_num in range(start_page_index, num_total_pages):
            page = doc.load_page(page_num)
            text = page.get_text("text").upper()

            if not text or text.isspace():
                continue

            # Detectar resumen
            if any(re.search(r'(?:^[ \t]*|\n[ \t]*)' + kw + r'\b', text) for kw in resumen_keywords):
                paginas_resumen.append(page_num)

            # Detectar bibliografía
            if any(re.search(r'(?:^[ \t]*|\n[ \t]*)' + kw + r'\b', text) for kw in biblio_keywords):
                paginas_biblio.append(page_num)

        doc.close()

    except Exception as e:
        print(f"WARN (detectar_resumen_biblio): Error procesando {pdf_path}: {e}")

    return paginas_resumen, paginas_biblio


def detect_image_regions_on_page(
    page: fitz.Page,
    merge_close_distance: int = 5,
    min_area: int = 1000,
    detect_drawings: bool = False,
    debug: bool = False
) -> list:
    """
    Detecta regiones probables de imágenes (y opcionalmente dibujos vectoriales)
    en una página de PyMuPDF, retornando bounding boxes fusionadas y filtradas.

    Args:
        page (fitz.Page): Página de PyMuPDF sobre la que se detectan imágenes.
        merge_close_distance (int): Distancia máxima (en puntos) para fusionar
            bounding boxes que se solapan o están muy cerca.
        min_area (int): Área mínima (en puntos^2) para no descartar regiones pequeñas.
        detect_drawings (bool): Si True, intentará detectar regiones vectoriales
            (get_drawings()) y tratarlas como imágenes.
        debug (bool): Si True, muestra mensajes de debug.

    Returns:
        list[dict]: Lista de regiones detectadas, cada una con:
            {
              "bbox": (x0, y0, x1, y1),
              "type": "image" | "drawing"
            }
    """
    all_regions = []
    try:
        # --------------------------------------------------------
        # 1. DETECCIÓN DE IMÁGENES BITMAP
        # --------------------------------------------------------
        images_info = page.get_images(full=True)
        for img_info in images_info:
            xref = img_info[0]
            if xref == 0:
                continue  # ignorar imágenes inline o inválidas
            try:
                # Obtener los rectángulos donde se dibuja esta imagen (puede haber varios)
                img_rects = page.get_image_rects(xref)
                for rect in img_rects:
                    bbox = rect.irect  # (x0, y0, x1, y1) con coords enteras
                    x0, y0, x1, y1 = bbox
                    area = (x1 - x0) * (y1 - y0)
                    if area >= min_area:
                        all_regions.append({"bbox": bbox, "type": "image"})
                    elif debug:
                        print(f"DEBUG: Descartando imagen muy pequeña bbox={bbox}, area={area}")
            except Exception as err_rects:
                if debug:
                    print(f"DEBUG: No se pudo obtener rects de imagen xref={xref}: {err_rects}")

        # --------------------------------------------------------
        # 2. DETECCIÓN DE "DRAWINGS" VECTORIALES (OPCIONAL)
        # --------------------------------------------------------
        if detect_drawings:
            try:
                drawings = page.get_drawings()
                for d in drawings:
                    # 'type' puede ser: 'l' (line), 're' (rectangle),
                    # 'f' (fill?), 'cs' (curves?), etc.
                    # Ajusta según tus necesidades de filtrado.
                    # Aquí descartamos líneas simples:
                    if d['type'] == 'l':
                        continue
                    bbox = d['rect'].irect
                    x0, y0, x1, y1 = bbox
                    area = (x1 - x0) * (y1 - y0)
                    if area >= min_area:
                        all_regions.append({"bbox": bbox, "type": "drawing"})
                    elif debug:
                        print(f"DEBUG: Descartando dibujo pequeño bbox={bbox}, area={area}")
            except Exception as err_draw:
                if debug:
                    print(f"DEBUG: Error detectando dibujos vectoriales: {err_draw}")

        # --------------------------------------------------------
        # 3. FUSIÓN DE BBOXES CERCANOS O SOLAPADOS
        # --------------------------------------------------------
        merged_regions = _merge_bounding_boxes(all_regions, merge_close_distance, debug=debug)

        if debug:
            print(f"DEBUG: detect_image_regions_on_page => {len(all_regions)} sin fusionar, {len(merged_regions)} tras fusión")

        return merged_regions

    except Exception as e:
        print(f"WARN: Error detectando imágenes/dibujos en página: {e}")
        return []

def detectar_paginas_indice(pdf_path, max_paginas_a_revisar=None, umbral_min_lineas=5):
    """
    Intenta detectar las páginas del índice (Tabla de Contenido) en un PDF.

    Utiliza heurísticas mejoradas basadas en patrones de texto comunes,
    combinando numeración jerárquica, palabras clave y opcionalmente
    la presencia de números de página al final de la línea.

    Args:
        pdf_path (str): Ruta al archivo PDF.
        max_paginas_a_revisar (int): Número máximo de páginas iniciales a revisar.
        umbral_min_lineas (int): Mínimo de líneas de texto requeridas en una página
                                 para siquiera considerarla como índice.

    Returns:
        list: Una lista de índices de página (basados en 0) que probablemente
              contienen el índice. Lista vacía si no se detecta ninguno o hay error.
    """
    paginas_indice_detectadas = []
    if not os.path.exists(pdf_path):
         print(f"ERROR: (detectar_paginas_indice) Archivo no encontrado: {pdf_path}")
         return paginas_indice_detectadas

    doc = None
    try:
        doc = fitz.open(pdf_path)
    except Exception as e:
        print(f"WARN: (detectar_paginas_indice) Error al abrir PDF '{pdf_path}': {e}")
        return paginas_indice_detectadas

    # --- Heurísticas ---
    # Regex para numeración como 1., 1.1, 1.1.1., CAPÍTULO 1, TEMA 2, etc. (más flexible)
    patron_numeracion_jerarquica = re.compile(
        r"^\s*([0-9]+(\.[0-9]+)*\.?\s+|"  # 1., 1.1, 1.1.
        r"(CAP[IÍ]TULO|TEMA|SECCI[OÓ]N|PARTE)\s+[0-9IVXLCDM]+\b\.?\s*).*",
        re.IGNORECASE
    )
    # Regex para palabras clave comunes en índices/sumarios (usando search)
    patron_palabras_clave = re.compile(
        r"^\s*(INTRODUCCI[OÓ]N|PR[OÓ]LOGO|CONCLUSI[OÓ]N|EP[IÍ]LOGO|BIBLIOGRAF[IÍ]A|WEBGRAF[IÍ]A|REFERENCIAS|RESUMEN|[IÍ]NDICE|CONTENIDO|SUMARIO|ANEXO|GLOSARIO)\b",
        re.IGNORECASE
    )
    # Regex para líneas que probablemente terminan en un número de página (puede estar precedido por puntos o espacios)
    patron_linea_con_pagina = re.compile(r".*[.\s]\s*(\d+)\s*$")

    num_paginas_a_escanear = min(max_paginas_a_revisar, doc.page_count)
    posible_indice_activo = False # Flag para detectar índices multi-página

    print(f"INFO: Escaneando hasta {num_paginas_a_escanear} páginas para índice en '{os.path.basename(pdf_path)}'")

    for num_pagina in range(num_paginas_a_escanear):
        try:
            pagina = doc.load_page(num_pagina)
            # Usar bloques puede ser un poco más robusto para la separación de líneas
            bloques = pagina.get_text("blocks")
            lineas = []
            for b in bloques:
                # b[4] contiene el texto del bloque, puede tener \n internos
                block_text = b[4]
                # Dividir por nueva línea y limpiar
                lineas.extend(line.strip() for line in block_text.split('\n') if line.strip())

            num_total_lineas = len(lineas)

            # Ignorar páginas casi vacías o portadas detectadas
            if num_total_lineas < umbral_min_lineas or is_likely_cover("\n".join(lineas), num_pagina, doc.page_count):
                # print(f"DEBUG P{num_pagina+1}: Ignorada (líneas={num_total_lineas} < {umbral_min_lineas} or portada)")
                posible_indice_activo = False # Si no es índice, rompe la cadena
                continue

            contador_lineas_patron = 0
            contador_palabras_clave = 0
            contador_lineas_con_pagina = 0

            for linea in lineas:
                if patron_numeracion_jerarquica.match(linea):
                    contador_lineas_patron += 1
                # Usamos search para palabras clave, más flexible a indentación
                if patron_palabras_clave.search(linea):
                    contador_palabras_clave += 1
                if patron_linea_con_pagina.match(linea):
                    # Verificación adicional: asegurarse de que el número no sea parte de la numeración inicial
                    match_num_inicial = patron_numeracion_jerarquica.match(linea)
                    num_final_match = patron_linea_con_pagina.match(linea)
                    if num_final_match:
                         num_final_str = num_final_match.group(1)
                         # Evitar contar si el número final es el mismo que el inicial (p.ej., "1. Título 1")
                         if not (match_num_inicial and linea.strip().endswith(num_final_str) and len(linea.split()) < 4):
                              contador_lineas_con_pagina += 1


            ratio_lineas_patron = contador_lineas_patron / num_total_lineas
            ratio_lineas_con_pagina = contador_lineas_con_pagina / num_total_lineas

            # --- Lógica de Decisión Mejorada ---
            es_pagina_indice = False
            score = 0.0

            # Puntuación base por estructura de numeración (alta importancia)
            score += ratio_lineas_patron * 0.6

            # Puntuación por líneas terminando en número (media importancia)
            score += ratio_lineas_con_pagina * 0.3

            # Bonus por presencia de palabras clave (menor importancia individual, pero ayuda)
            if contador_palabras_clave > 0:
                score += 0.1 # Bonus fijo pequeño si hay al menos una
            if contador_palabras_clave > 2:
                score += 0.1 # Bonus adicional si hay varias

            # Umbral base para considerar índice
            umbral_score_base = 0.25 # Ajustar según sea necesario

            # Umbral más bajo si la página anterior fue índice (continuación)
            umbral_score_continuacion = 0.18

            if posible_indice_activo:
                if score >= umbral_score_continuacion:
                    es_pagina_indice = True
            else:
                 if score >= umbral_score_base:
                    es_pagina_indice = True

            # Refinamiento: Una página con muchas palabras clave pero CERO estructura podría ser un falso positivo
            # O una página con ALTA estructura pero pocas líneas podría no serlo.
            # (La comprobación de umbral_min_lineas ya ayuda con lo segundo)
            if contador_palabras_clave > 1 and contador_lineas_patron == 0 and contador_lineas_con_pagina == 0:
                 # Si SOLO tiene palabras clave y ninguna otra estructura, probablemente no sea índice (podría ser intro/conclusión)
                 # A menos que tenga MUCHAS líneas con palabras clave? Podría ser un índice simple.
                 if num_total_lineas > 10 and (contador_palabras_clave / num_total_lineas > 0.3): # Si >30% de lineas son keywords
                     pass # Probablemente un índice simple basado en keywords, mantener es_pagina_indice si score fue suficiente
                 else:
                     es_pagina_indice = False # Descartar si no cumple la condición anterior


            # DEBUGGING INTERNO
            print(f"  Pág {num_pagina + 1}: Lines={num_total_lineas}, "
                  f"RatioPatron={ratio_lineas_patron:.2f} ({contador_lineas_patron}), "
                  f"Keywords={contador_palabras_clave}, "
                  f"RatioPgNum={ratio_lineas_con_pagina:.2f} ({contador_lineas_con_pagina}), "
                  f"Score={score:.3f} -> Índice? {es_pagina_indice} (ActivoPrev? {posible_indice_activo})")

            if es_pagina_indice:
                if num_pagina not in paginas_indice_detectadas:
                     paginas_indice_detectadas.append(num_pagina)
                posible_indice_activo = True
            else:
                # Si la página no cumple, se rompe la posible cadena de índice
                posible_indice_activo = False

        except Exception as e:
            print(f"WARN: (detectar_paginas_indice) Error procesando página {num_pagina} del PDF: {e}")
            posible_indice_activo = False # Resetear en caso de error
            continue

    if doc:
        doc.close()

    # Post-procesamiento: a veces puede detectar una página suelta entre otras.
    # Si tenemos [0, 2], pero no 1, es menos probable que 2 sea índice.
    # Podríamos requerir bloques contiguos, pero por simplicidad lo dejamos así por ahora.

    print(f"INFO: Páginas de índice detectadas: {[p+1 for p in paginas_indice_detectadas]}") # Mostrar páginas base 1
    return paginas_indice_detectadas

def detect_formulas_in_text(text):
    """
    Detecta fórmulas tipo LaTeX en un texto (cuando existen los delimitadores):
      - Inline: $ ... $
      - Display: $$ ... $$
      - Entorno: \(...\), \[...\]
      - Entornos: \begin{equation}, \begin{align}, etc.

    Retorna lista de dicts con campos:
      {
        "latex_content": str,
        "full_match": str,
        "delimiter_type": str,  # '$', '$$', '\(\)', '\[\]', 'equation', etc.
        "start_pos": int,
        "end_pos": int
      }
    """
    # Para ignorar dólares escapados (\$), reemplazamos temporalmente
    # por algo inofensivo, de modo que no interfieran en la búsqueda.
    text_ignoring_escaped = re.sub(r'\\\$', '__ESCAPED_DOLLAR__', text)

    formulas = []

    # Patrones de LaTeX
    inline_pattern = re.compile(r'\$([^\$]+?)\$', re.DOTALL)
    display_pattern = re.compile(r'\$\$([^\$]+?)\$\$', re.DOTALL)
    backslash_paren_pattern = re.compile(r'\\\((.*?)\\\)', re.DOTALL)
    backslash_bracket_pattern = re.compile(r'\\\[(.*?)\\\]', re.DOTALL)
    begin_env_pattern = re.compile(
        r'\\begin\{(equation|align|align\*|gather|gather\*|multline|multline\*)\}(.*?)\\end\{\1\}',
        re.DOTALL
    )

    pattern_list = [
        (display_pattern, lambda m: '$$'),
        (inline_pattern, lambda m: '$'),
        (backslash_paren_pattern, lambda m: r'\(\)'),
        (backslash_bracket_pattern, lambda m: r'\[\]'),
        (begin_env_pattern, lambda m: m.group(1))
    ]

    for pattern, delim_fn in pattern_list:
        for match in pattern.finditer(text_ignoring_escaped):
            content = match.group(1).strip()
            if content:
                formulas.append({
                    "latex_content": content,
                    "full_match": match.group(0),
                    "delimiter_type": delim_fn(match),
                    "start_pos": match.start(),
                    "end_pos": match.end()
                })

    # Ordenar por posición
    formulas.sort(key=lambda f: f['start_pos'])
    return formulas

# ----------------------------------------------------------------
# 2. Detector heurístico para expresiones matemáticas “simples”
#    (cuando NO hay delimitadores LaTeX en el PDF)
# ----------------------------------------------------------------
def detect_heuristic_math_expressions(text):
    """
    Retorna True si 'text' aparenta contener “algo matemático” basado en
    patrones heurísticos: subíndices, superíndices, símbolos griegos, etc.
    """

    # Regex para subíndices/superíndices tipo x_2, x^2, x_{ij}, x^{2n+1}, etc.
    subsuper_regex = re.compile(
        r'[A-Za-z0-9]\s*(\^|_)\s*(\{[^}]+\}|\w+|\d+)',  # Ej: x^2, f_{n+1}
    )

    # Regex para caracteres matemáticos comunes en Unicode
    # (podrías ampliar más la lista si lo deseas).
    math_symbols_regex = re.compile(
        r'[∂∑∏√∞±≤≥≈≠÷⋅→←⇒⇔∆∫∏∑−∂]'  # etc.
    )

    # Regex para funciones con paréntesis: f(x), g(x,y), sin(2x), etc.
    function_call_regex = re.compile(
        r'[A-Za-z][A-Za-z0-9_]*\s*\([^()]*\)'
    )

    # Verifica si alguno de los patrones se cumple
    if subsuper_regex.search(text):
        return True
    if math_symbols_regex.search(text):
        return True
    if function_call_regex.search(text):
        return True

    return False

def extract_and_clean_pdf_smart_stem(pdf_path,
                                     use_ocr_threshold=50,
                                     language='spa', # 'language' no se usa directamente aquí, quizás en OCR
                                     max_index_pages_to_scan=15,
                                     max_summary_biblio_pages_to_scan=10,
                                     debug_prints=False): # Añadido parámetro debug_prints
    """
    Extrae texto de PDF (modo STEM), detecta fórmulas/imágenes,
    omite portadas/índices, limpia texto, y elimina solo el bloque de
    bibliografía sin eliminar páginas enteras.

    - Aplica detect_formulas_in_text() para ver si hay LaTeX literal.
    - Marca si el texto parece contener expresiones matemáticas (heurística).
    - Detecta secciones matemáticas (teorema, definición, demostración, etc.)
      a nivel global.
    """
    all_formulas_detected = []
    all_image_regions = {}
    valid_pages_text = []
    omitted_pages_info = []
    # debug_prints = False # Se recibe como parámetro ahora

    try:
        if not os.path.exists(pdf_path):
            print(f"Error GRAVE: No se encontró el archivo PDF: {pdf_path}")
            # Devolver None o un dict vacío con error es mejor que solo None
            return {"error": f"File not found: {pdf_path}", "cleaned_text": "", "detected_formulas": [], "detected_image_regions": {}, "omitted_pages": [], "heuristic_math_detected": False, "detected_math_sections": {}}


        pdf_basename = os.path.basename(pdf_path)

        # --- Pre-detección de ÍNDICE ---
        if debug_prints: print(f"DEBUG ({pdf_basename}): Pre-detectando páginas de índice (hasta {max_index_pages_to_scan} págs)...")
        paginas_indice_detectadas = detectar_paginas_indice(
            pdf_path, max_paginas_a_revisar=max_index_pages_to_scan
        )
        if paginas_indice_detectadas:
            if debug_prints: print(f"DEBUG ({pdf_basename}): Posibles páginas de ÍNDICE (0-based): {paginas_indice_detectadas}")

        # --- Pre-detección de RESUMEN/BIBLIO ---
        if debug_prints: print(f"DEBUG ({pdf_basename}): Pre-detectando págs Resumen/Biblio (últimas {max_summary_biblio_pages_to_scan})...")
        paginas_resumen_detectadas, paginas_biblio_detectadas = detectar_paginas_resumen_biblio(
            pdf_path, max_paginas_finales_a_revisar=max_summary_biblio_pages_to_scan
        )
        if paginas_resumen_detectadas:
            if debug_prints: print(f"DEBUG ({pdf_basename}): Páginas con posible RESUMEN (0-based): {paginas_resumen_detectadas}")
        if paginas_biblio_detectadas:
            if debug_prints: print(f"DEBUG ({pdf_basename}): Páginas con posible BIBLIOGRAFÍA (0-based): {paginas_biblio_detectadas}")

        # --- Crear conjunto de páginas a omitir solo para índice ---
        # Convertir a 0-based si las funciones de detección devuelven 1-based
        # Asumiendo que devuelven 0-based:
        paginas_a_omitir_previamente = set(paginas_indice_detectadas)

        doc = fitz.open(pdf_path)
        num_total_pages = len(doc)
        if debug_prints: print(f"DEBUG ({pdf_basename}): Procesando {num_total_pages} páginas (Modo STEM).")

        for page_num in range(num_total_pages):
            page_num_real = page_num + 1 # Para logs y referencias (1-based)
            if debug_prints: print(f"DEBUG ({pdf_basename}): Procesando pág {page_num_real}/{num_total_pages}...")

            # --- Omisión de páginas índice ---
            if page_num in paginas_a_omitir_previamente:
                reason = "Índice (pre-detectado)"
                omitted_pages_info.append((page_num_real, reason))
                if debug_prints:
                    print(f"  -> OMITIDA ({reason}).")
                continue

            # --- Extracción básica de texto ---
            page_raw_text = ""
            page_raw_text_strip = ""
            try:
                page = doc.load_page(page_num) # Cargar página dentro del bucle
                page_raw_text = page.get_text("text", sort=True)
                page_raw_text_strip = page_raw_text.strip() if page_raw_text else ""
                if debug_prints and not page_raw_text_strip:
                    print(f"  -> WARN: get_text devolvió vacío o solo espacios.")
            except Exception as getTextErr:
                print(f"WARN ({pdf_basename}): get_text falló pág {page_num_real}: {getTextErr}.")
                omitted_pages_info.append((page_num_real, f"Error get_text: {getTextErr}"))
                continue # Saltar página si falla la extracción básica

            # --- Lógica de OCR (Opcional, si se requiere) ---
            used_ocr = False
            if not page_raw_text_strip or len(page_raw_text_strip) < use_ocr_threshold:
                # Aquí iría la llamada a una función OCR si decides implementarla
                # page_raw_text_ocr = apply_ocr_to_page(page, language=language)
                # if page_raw_text_ocr and len(page_raw_text_ocr.strip()) > len(page_raw_text_strip):
                #     page_raw_text = page_raw_text_ocr
                #     page_raw_text_strip = page_raw_text.strip()
                #     used_ocr = True
                #     if debug_prints: print(f"  -> INFO: OCR aplicado (resultado > {use_ocr_threshold} chars).")
                # elif debug_prints:
                #     print(f"  -> INFO: Texto < {use_ocr_threshold} chars, OCR no aplicado o sin mejora.")
                pass # Placeholder para OCR

            # --- Comprobación de texto vacío (Post-OCR si aplica) ---
            if not page_raw_text_strip:
                reason = "Sin texto válido (post-OCR)" if used_ocr else "Sin texto válido"
                omitted_pages_info.append((page_num_real, reason))
                if debug_prints: print(f"  -> OMITIDA ({reason}).")
                continue

            # --- Heurística de portada ---
            if is_likely_cover(page_raw_text_strip, page_num, num_total_pages):
                reason = "Portada (heurística)"
                omitted_pages_info.append((page_num_real, reason))
                if debug_prints: print(f"  -> OMITIDA ({reason}).")
                continue

            # --- Cortar bibliografía si corresponde (Solo si la página fue pre-detectada) ---
            final_page_text = page_raw_text_strip # Usar texto strip para búsqueda
            original_length_before_bib_cut = len(final_page_text)
            bibliography_cut_applied = False

            if page_num in paginas_biblio_detectadas:
                # Buscar keywords en mayúsculas para robustez, pero cortar el original
                biblio_keywords_regex = r'^\s*(BIBLIOGRAFÍA|REFERENCIAS|WEBGRAFÍA)\s*$' # Más específico, inicio de línea
                # Intentar buscar desde el final de la página hacia atrás podría ser más robusto
                lines = final_page_text.splitlines()
                cut_index = -1
                for i in range(len(lines) - 1, -1, -1):
                     if re.search(biblio_keywords_regex, lines[i].strip().upper()):
                         # Encontrar la posición de inicio de esta línea en el texto original
                         try:
                            cut_index = final_page_text.rindex(lines[i])
                            break
                         except ValueError:
                            pass # Seguir buscando si la línea no se encuentra exactamente

                if cut_index != -1:
                    final_page_text = final_page_text[:cut_index].strip()
                    bibliography_cut_applied = True
                    if debug_prints:
                        print(f"  -> INFO: Texto cortado por keyword de bibliografía encontrada.")

                    # Si tras cortar no queda texto, omitir la página
                    if not final_page_text:
                        reason = "Texto eliminado por contenido de bibliografía"
                        omitted_pages_info.append((page_num_real, reason))
                        if debug_prints: print(f"  -> OMITIDA ({reason}).")
                        continue

            # --- Detección de fórmulas e imágenes (en el texto final de la página) ---
            page_formulas = detect_formulas_in_text(final_page_text) # Usar texto potencialmente cortado
            if page_formulas:
                # Ajustar posiciones si se cortó la bibliografía? No, las pos son relativas a final_page_text
                all_formulas_detected.extend(page_formulas)
                if debug_prints: print(f"  -> INFO: Detectadas {len(page_formulas)} fórmulas LaTeX.")

            # Detectar imágenes (asumiendo que la función existe)
            page_image_bboxes = detect_image_regions_on_page(page)
            if page_image_bboxes:
                all_image_regions[page_num_real] = page_image_bboxes
                if debug_prints: print(f"  -> INFO: Detectadas {len(page_image_bboxes)} regiones de imagen.")

            # --- Página Aceptada (añadir texto final) ---
            valid_pages_text.append(final_page_text)
            if debug_prints: print(f"  -> ACEPTADA (len: {len(final_page_text)} chars).")

        # --- Fin del bucle de páginas ---
        doc.close()

        # --- Resumen de omisiones ---
        print("\n" + "-"*20 + f" Resumen Omisiones ({pdf_basename}) " + "-"*20)
        if not omitted_pages_info:
            print("INFO: No se omitió ninguna página.")
        else:
            omitted_by_reason = defaultdict(list)
            for page, reason in omitted_pages_info:
                omitted_by_reason[reason].append(page)
            print(f"INFO: Omitidas {len(omitted_pages_info)}/{num_total_pages} páginas:")
            for reason, pages in sorted(omitted_by_reason.items()):
                pages.sort()
                # Agrupar páginas consecutivas para mejor lectura
                grouped_pages = []
                if pages:
                    start_range = pages[0]
                    end_range = pages[0]
                    for i in range(1, len(pages)):
                        if pages[i] == end_range + 1:
                            end_range = pages[i]
                        else:
                            if start_range == end_range:
                                grouped_pages.append(str(start_range))
                            else:
                                grouped_pages.append(f"{start_range}-{end_range}")
                            start_range = end_range = pages[i]
                    # Añadir el último rango/página
                    if start_range == end_range:
                        grouped_pages.append(str(start_range))
                    else:
                        grouped_pages.append(f"{start_range}-{end_range}")
                print(f"  - Razón: '{reason}', Páginas: {', '.join(grouped_pages)}")
        print("-"*(42 + len(f" Resumen Omisiones ({pdf_basename}) ")))

        # --- Limpieza final del texto concatenado ---
        if not valid_pages_text:
            print(f"ERROR ({pdf_basename}): No se aceptó ninguna página válida.")
            return {
                "cleaned_text": "",
                "detected_formulas": [],
                "detected_image_regions": {},
                "omitted_pages": omitted_pages_info,
                "heuristic_math_detected": False,
                "detected_math_sections": {}
            }

        full_raw_text = "\n\n".join(valid_pages_text) # Unir páginas aceptadas
        if debug_prints: print(f"DEBUG ({pdf_basename}): {len(valid_pages_text)} págs aceptadas. Limpiando texto concatenado...")

        # Asumiendo que clean_pdf_text_robust está definido.
        cleaned_text = clean_pdf_text_robust(full_raw_text)
        if debug_prints: print(f"DEBUG ({pdf_basename}): Limpieza completada. Longitud final: {len(cleaned_text)} chars.")

        if not cleaned_text or cleaned_text.isspace():
            print(f"WARN ({pdf_basename}): Texto final limpio vacío.")
            # Devolver el estado aunque el texto esté vacío
            return {
                "cleaned_text": "",
                "detected_formulas": all_formulas_detected,
                "detected_image_regions": all_image_regions,
                "omitted_pages": omitted_pages_info,
                "heuristic_math_detected": False, # No hay texto para analizar
                "detected_math_sections": {}
            }

        # --- Análisis Global del Texto Limpio ---
        # Heurística global de fórmulas en el texto
        has_any_heuristic_math = detect_heuristic_math_expressions(cleaned_text)
        if debug_prints: print(f"DEBUG ({pdf_basename}): Heurística matemática global detectada: {has_any_heuristic_math}")

        # Detección de secciones matemáticas (teorema, definición, etc.)
        math_sections_info = detectar_secciones_matematicas(cleaned_text)
        if debug_prints: print(f"DEBUG ({pdf_basename}): Secciones matemáticas detectadas: {math_sections_info}")


        # --- Devolver Resultados ---
        return {
            "cleaned_text": cleaned_text,
            "detected_formulas": all_formulas_detected,       # LaTeX literal
            "detected_image_regions": all_image_regions,
            "omitted_pages": omitted_pages_info,
            "heuristic_math_detected": has_any_heuristic_math,  # True/False si hay expresiones "sospechosas"
            "detected_math_sections": math_sections_info        # Dict con las listas y booleanos
        }

    except FileNotFoundError:
 # Ser más específico con la excepción
        print(f"Error GRAVE: No se encontró el PDF: {pdf_path}")
        return {"error": f"File not found: {pdf_path}", "cleaned_text": "", "detected_formulas": [], "detected_image_regions": {}, "omitted_pages": [], "heuristic_math_detected": False, "detected_math_sections": {}}
    except Exception as e:
        print(f"Error GRAVE procesando PDF {os.path.basename(pdf_path)} ({type(e).__name__}): {e}")
        traceback.print_exc()
        return {"error": f"Processing error: {e}", "cleaned_text": "", "detected_formulas": [], "detected_image_regions": {}, "omitted_pages": [], "heuristic_math_detected": False, "detected_math_sections": {}}



def chunk_text_by_sentences(text, max_words=200, overlap_sentences=1):
    """Divide el texto en chunks basados en frases, con límite de palabras y solapamiento."""
    if not text: return []
    overlap_sentences = max(0, overlap_sentences)
    try:
        sentences = sent_tokenize(text, language='spanish')
        if not sentences: # Si sent_tokenize devuelve vacío, usar fallback
             raise ValueError("sent_tokenize devolvió lista vacía")
    except Exception as e:
        # print(f"DEBUG: sent_tokenize falló o vacío ({e}), usando fallback por saltos de línea.")
        # Fallback: dividir por dobles saltos de línea, luego simples
        sentences = [p.strip() for p in text.split('\n\n') if p.strip()]
        if not sentences:
             sentences = [p.strip() for p in text.split('\n') if p.strip()]
        # Fallback final: dividir por puntos si todo lo demás falla
        if not sentences:
             sentences = [s.strip() + '.' for s in text.split('.') if s.strip()]

    chunks = []
    i = 0
    n_sentences = len(sentences)

    while i < n_sentences:
        current_chunk_sentences = []
        word_count = 0
        start_sentence_index = i

        j = i
        while j < n_sentences:
            current_sentence = sentences[j].strip()
            if not current_sentence:
                j += 1; continue

            sentence_words = len(current_sentence.split())

            # Añadir la primera frase siempre, o si cabe
            if not current_chunk_sentences or (word_count + sentence_words <= max_words):
                current_chunk_sentences.append(current_sentence)
                word_count += sentence_words
                j += 1
            else:
                break # La frase actual no cabe

        # Procesar el chunk acumulado
        if current_chunk_sentences:
            chunks.append(" ".join(current_chunk_sentences))
            # Calcular índice de inicio del siguiente chunk con solapamiento
            # Retroceder `overlap_sentences` desde `j` (la frase que no cupo)
            # pero sin ir más atrás que `start_sentence_index + 1`
            next_start_index = max(start_sentence_index + 1, j - overlap_sentences)
            i = min(next_start_index, n_sentences) # Asegurar que no se pase del final
        # Manejar caso de frase inicial demasiado larga
        elif j < n_sentences and j == i: # No se avanzó, la primera frase ya era > max_words
            long_sentence = sentences[j].strip()
            if long_sentence:
                 chunks.append(long_sentence)
                 # print(f"DEBUG: Frase inicial larga (idx {j}) añadida como chunk.")
            i = j + 1 # Avanzar al siguiente
        # Caso raro: no se añadieron frases y no era la inicial (ej, bucle infinito?)
        elif j < n_sentences:
             # print(f"DEBUG: Avanzando índice de {i} a {j} sin añadir chunk.")
             i = j
        else: # Fin de las frases
            break
    return chunks

# --- Celda 3: Funciones Auxiliares ---
# --- Celda 3: Funciones Auxiliares ---

def simple_tokenizer(text):
    """Tokenizador simple: minúsculas y split por espacios."""
    if not isinstance(text, str):
        return []
    return text.lower().split()

# Opcional: Tokenizador más robusto con NLTK (requiere descargas en Celda 1)
# def nltk_tokenizer(text):
#     """Tokenizador con NLTK: minúsculas, palabras, sin puntuación ni stopwords."""
#     if not isinstance(text, str):
#         return []
#     words = word_tokenize(text.lower(), language='spanish')
#     # Asegúrate que spanish_stopwords está definida si descomentas esto
#     # return [word for word in words if word.isalnum() and word not in spanish_stopwords]
#     return [word for word in words if word.isalnum()] # Sin stopwords

# Elige tu tokenizador preferido aquí (¡asegúrate que la función existe!)
tokenizer_for_bm25 = simple_tokenizer
# tokenizer_for_bm25 = nltk_tokenizer # Si prefieres NLTK


def norm_score(score, min_val, max_val):
    """
    Normaliza un score a un rango [0, 1].
    Maneja el caso donde min_val == max_val para evitar división por cero.
    """
    if min_val == max_val:
        # Si todos los scores son iguales, podemos devolver 0.5 (neutral) o 1 si el score es ese valor, o 0.
        # Devolver 0 si min_val == max_val y score == min_val (o cualquier score ya que son todos iguales)
        # o 0.5 para indicar que no hay varianza. Elegiremos 0.5 como un valor neutral.
        # Otra opción es devolver 1.0 si solo hay un resultado y es positivo, o 0.0 si es 0.
        # O, si solo hay un elemento, su score normalizado puede ser 1.
        return 1.0 if score > 0 else 0.0 # Si hay un solo score y es > 0, es el "mejor"
    if max_val - min_val == 0: # Otra forma de chequear división por cero
        return 0.5 # O 1.0 si el score es el único valor
    return (score - min_val) / (max_val - min_val)

import re

def calcular_pesos_dinamicos(query: str, subject: str = None) -> tuple[float, float]:
    """
    Analiza la query educativa y el tema (opcional) y ajusta pesos entre BM25 y Embeddings.
    Devuelve (peso_bm25, peso_emb).
    """
    query_lower = query.lower()
    query_original = query # Para checks de mayúsculas

    # --- Pesos Base ---
    peso_bm25 = 0.4
    peso_emb = 0.6
    razon_principal = "Default (ligero sesgo Embedding)"
    detalles_razon = []

    # --- 1. Indicadores de ALTA ESPECIFICIDAD (Prioridad Alta para BM25) ---

    # 1.1. Citas exactas (texto entre comillas)
    if re.search(r'"[^"]+"', query_original): # Busca texto entre comillas dobles
        peso_bm25 = 0.85
        peso_emb = 0.15
        razon_principal = "Cita Exacta"
        detalles_razon.append("BM25 priorizado para coincidencia literal.")
        print_pesos_info(razon_principal, detalles_razon, peso_bm25, peso_emb)
        return peso_bm25, peso_emb

    # 1.bis. Definición de Término Clave Específico (Ej: "elipsis", "hipérbaton")
    definicion_keywords_specific_term = [
        "define", "definición de", "definir", "significa",
        "qué es", "que es", "cuál es el significado de",
        "concepto de"
    ]
    term_to_define_specific = ""
    for keyword in definicion_keywords_specific_term:
        # Patrón para "keyword X" o "keyword 'X'" o "keyword "X""
        # o para "X keyword" (menos común para estas keywords pero podría pasar)
        # Priorizamos "keyword X"
        if query_lower.startswith(keyword + " "):
            potential_term = query_lower[len(keyword)+1:].strip()
            # Quitar comillas y signos de interrogación del término
            potential_term = re.sub(r"['\"?¿!¡]$", "", potential_term).strip()
            potential_term = re.sub(r"^['\"]", "", potential_term).strip()

            # Si la query original tenía el término entre comillas, es buena señal
            if f"'{potential_term}'" in query_original or f'"{potential_term}"' in query_original:
                 term_to_define_specific = potential_term
                 break
            # Si no, tomarlo si es corto
            elif len(potential_term.split()) <= 3:
                 term_to_define_specific = potential_term
                 break

    if term_to_define_specific and len(term_to_define_specific.split()) <= 3 and len(query.split()) < 8 : # Término corto, query no demasiado larga
        # Evitar que una pregunta conceptual larga que casualmente empieza con "qué es la vida..." caiga aquí
        # Si la query es más larga, es probable que sea más conceptual.
        peso_bm25 = 0.80 # Alta prioridad para BM25 para encontrar el término exacto
        peso_emb = 0.20
        razon_principal = "Definición de Término Clave Específico"
        detalles_razon.append(f"Término detectado: '{term_to_define_specific}'. BM25 fuertemente priorizado.")
        print_pesos_info(razon_principal, detalles_razon, peso_bm25, peso_emb)
        return peso_bm25, peso_emb


    # 1.2. Búsqueda de Leyes, Artículos, Teoremas específicos
    if re.search(r'\b(ley|artículo|teorema|postulado|axioma|principio)\s+([0-9]+|[xviíclmd]+|[A-Za-z\s]+)\b', query_lower, re.IGNORECASE):
        peso_bm25 = 0.75
        peso_emb = 0.25
        razon_principal = "Ley/Artículo/Teorema Específico"
        detalles_razon.append("BM25 priorizado para identificadores exactos.")
        print_pesos_info(razon_principal, detalles_razon, peso_bm25, peso_emb)
        return peso_bm25, peso_emb

    # 1.3. Fórmulas o Ecuaciones
    if re.search(r'\b[a-zA-Z]\s*=\s*[a-zA-Z0-9]|\b[a-zA-Z]\w*\([a-zA-Z\d,\s]*\)|[a-zA-Z]\w*_[a-zA-Z\d]|\w\^[2-9]\b', query_original):
        if subject in ["Física", "Biología", "Matemáticas", "Química"]: # Más probable que sea una fórmula
            peso_bm25 = 0.70
            peso_emb = 0.30
            razon_principal = "Posible Fórmula/Ecuación"
            detalles_razon.append(f"BM25 priorizado en {subject} para coincidencia estructural.")
            print_pesos_info(razon_principal, detalles_razon, peso_bm25, peso_emb)
            return peso_bm25, peso_emb

    # --- 2. Indicadores de ESPECIFICIDAD MEDIA (Favorecen BM25, pero con espacio para semántica) ---

    # 2.1. Nombres Propios
    nombres_propios_candidatos = re.findall(r'\b[A-ZÁÉÍÓÚÑ][a-záéíóúñ]{2,}(?:\s+[A-ZÁÉÍÓÚÑ][a-záéíóúñ]{1,})*\b', query_original)
    if nombres_propios_candidatos:
        if not (len(nombres_propios_candidatos) == 1 and query_original.startswith(nombres_propios_candidatos[0]) and len(query.split()) > 3):
            peso_bm25 = max(peso_bm25, 0.65) # Aumenta si el default era menor, o lo establece
            peso_emb = 1.0 - peso_bm25
            if razon_principal.startswith("Default"): razon_principal = "Nombre Propio Detectado"
            detalles_razon.append(f"Candidatos NP: {nombres_propios_candidatos}. BM25 priorizado.")

    # 2.2. Fechas, Años, Siglos
    if re.search(r'\b\d{3,4}\b', query_lower) or \
       re.search(r'\bsiglo\s+(?:[xviíclmd]+|[0-9]+)\b', query_lower) or \
       re.search(r'\b(año|fecha)\s+\d{1,4}\b', query_lower) or \
       re.search(r'\b\d{1,2}(?:/| de |-| del )\w+(?:/| de |-| del )\d{2,4}\b', query_lower):
        peso_bm25 = max(peso_bm25, 0.70)
        peso_emb = 1.0 - peso_bm25
        if razon_principal.startswith("Default") or "Nombre Propio" in razon_principal: razon_principal = "Fecha/Año/Siglo Detectado"
        detalles_razon.append("BM25 priorizado para especificidad temporal.")
        if subject == "Historia":
            peso_bm25 = max(peso_bm25, 0.75) # Aún más para Historia
            peso_emb = 1.0 - peso_bm25
            detalles_razon.append("Alta prioridad BM25 en Historia.")

    # 2.3. Acrónimos y Términos Técnicos Muy Específicos
    acronimos_candidatos = re.findall(r'\b[A-ZÁÉÍÓÚÑ]{2,}\b', query_original)
    if acronimos_candidatos and not query_original.isupper():
        if not (len(acronimos_candidatos) == 1 and query_original.startswith(acronimos_candidatos[0])):
            peso_bm25 = max(peso_bm25, 0.60)
            peso_emb = 1.0 - peso_bm25
            if razon_principal.startswith("Default") or "Nombre Propio" in razon_principal or "Fecha" in razon_principal:
                razon_principal = "Acrónimo/Término Técnico Específico Detectado"
            detalles_razon.append(f"Candidatos Acrónimo: {acronimos_candidatos}. BM25 con peso incrementado.")


    # --- 3. Indicadores de BÚSQUEDA DE DEFINICIONES (Equilibrio, si no es ya muy específico) ---
    # Esta regla se aplica si las de ALTA ESPECIFICIDAD (incluida 1.bis) no se activaron y retornaron.
    definicion_keywords_general = ["define", "definición de", "definir", "significa", "concepto de"]
    que_es_keywords_general = ["qué es", "que es", "cual es el significado de", "cuál es el significado de"]

    is_general_definition_request = False
    if any(keyword in query_lower for keyword in definicion_keywords_general) or \
       any(query_lower.startswith(keyword) for keyword in que_es_keywords_general):
        is_general_definition_request = True

    if is_general_definition_request:
        # Si ya se marcó como muy específico (nombre propio, fecha, acrónimo), mantenemos BM25 alto,
        # pero si la razón principal aún es "Default" o algo menos específico.
        if peso_bm25 < 0.6: # Solo ajusta si no es ya específico por reglas anteriores
            peso_bm25 = 0.55
            peso_emb = 0.45
            razon_principal = "Petición de Definición General"
            detalles_razon.append("Pesos ligeramente inclinados a BM25 para literalidad, pero con semántica.")
        else:
            detalles_razon.append("Petición de definición, pero query ya tenía especificidad media/alta.")


    # --- 4. Indicadores de CONCEPTUALIDAD (Prioridad para Embeddings) ---
    concept_keywords_strong = ["explica", "describe el proceso de", "analiza las causas de", "compara y contrasta",
                               "cuál es la importancia de", "interpreta", "relación entre", "impacto de",
                               "evolución de", "fundamentos de", "teoría de"]
    concept_keywords_medium = ["cómo funciona", "por qué ocurre", "cuáles son las características",
                               "tipos de", "función de", "origen de", "propiedades de"]

    is_conceptual = False
    conceptual_keyword_found = ""
    for keyword in concept_keywords_strong:
        if keyword in query_lower:
            is_conceptual = True
            conceptual_keyword_found = keyword
            detalles_razon.append(f"Palabra clave conceptual fuerte detectada: '{keyword}'.")
            break
    if not is_conceptual:
        for keyword in concept_keywords_medium:
            if keyword in query_lower:
                is_conceptual = True
                conceptual_keyword_found = keyword
                detalles_razon.append(f"Palabra clave conceptual media detectada: '{keyword}'.")
                break

    if is_conceptual:
        # Si es una pregunta conceptual sobre un término muy específico (ya capturado por NP, Fecha, Acrónimo)
        # Ej: "Explica el impacto de la Peste Negra" -> Peste Negra (NP) + Explica (Conceptual)
        if peso_bm25 >= 0.65 : # Ya era muy específico
            peso_bm25 = 0.55 # Mantenemos algo de BM25 para el término, pero damos espacio a la explicación
            peso_emb = 0.45
            razon_principal = "Pregunta Conceptual Muy Específica"
            detalles_razon.append(f"Término específico combinado con petición conceptual ('{conceptual_keyword_found}').")
        elif peso_bm25 >= 0.55 and peso_bm25 < 0.65: # Especificidad media
            peso_bm25 = 0.40
            peso_emb = 0.60
            razon_principal = "Pregunta Conceptual con Especificidad Media"
            detalles_razon.append(f"Término con especificidad media combinado con petición conceptual ('{conceptual_keyword_found}').")
        else: # Pregunta conceptual más general
            peso_bm25 = 0.25
            peso_emb = 0.75
            razon_principal = "Pregunta Conceptual General"
            detalles_razon.append(f"Mayor peso para Embeddings debido a '{conceptual_keyword_found}'.")


    # --- 5. Ajustes por Asignatura (si se proporciona y no hay una regla fuerte dominante) ---
    if subject and (razon_principal.startswith("Default") or "Petición de Definición General" in razon_principal):
        original_razon_principal = razon_principal # Guardar por si no se modifica
        if subject == "Lengua Castellana":
            if "analiza el poema" in query_lower or "figuras retóricas" in query_lower or "estilo de" in query_lower or "comentario de texto" in query_lower:
                peso_bm25 = 0.3
                peso_emb = 0.7
                razon_principal = f"Conceptual (Lengua - Análisis Literario)"
            elif "regla gramatical" in query_lower or "ortografía de" in query_lower or "sintaxis de" in query_lower:
                peso_bm25 = 0.6
                peso_emb = 0.4
                razon_principal = f"Específico (Lengua - Gramática/Ortografía)"
        elif subject == "Historia":
            if "batalla de" in query_lower or "tratado de" in query_lower or "reinado de" in query_lower or "guerra de" in query_lower:
                if peso_bm25 < 0.65: # Solo si no fue ya capturado por NP/Fecha con alta prioridad
                    peso_bm25 = 0.65
                    peso_emb = 0.35
                    razon_principal = f"Evento Específico (Historia)"

        if original_razon_principal != razon_principal: # Si se aplicó una regla de asignatura
             detalles_razon.append(f"Ajuste por asignatura '{subject}'.")


    # --- 6. Ajuste final por longitud de la query (si aún es default o poco definido) ---
    # Se aplica si ninguna regla fuerte o de especificidad media/conceptual clara dominó
    if razon_principal.startswith("Default") or \
       ("Petición de Definición General" in razon_principal and peso_bm25 == 0.55) or \
       (peso_bm25 >= 0.35 and peso_bm25 <= 0.45 and not is_conceptual): # Default o ligeramente inclinado a Emb sin ser conceptual fuerte

        num_words_query = len(query.split())
        if num_words_query > 10:
            peso_bm25 = 0.30
            peso_emb = 0.70
            razon_principal = "Ajuste por Longitud (Larga -> Conceptual)"
            detalles_razon.append(f"Query larga ({num_words_query} palabras), favoreciendo semántica.")
        elif num_words_query < 4:
            peso_bm25 = 0.50 # Si era default (0.4), lo sube un poco para términos cortos
            peso_emb = 0.50
            razon_principal = "Ajuste por Longitud (Corta -> Equilibrio/Específica)"
            detalles_razon.append(f"Query corta ({num_words_query} palabras), buscando equilibrio o término.")


    print_pesos_info(razon_principal, detalles_razon, peso_bm25, peso_emb)
    return peso_bm25, peso_emb

def print_pesos_info(razon_principal, detalles_razon, peso_bm25, peso_emb):
    """Función auxiliar para imprimir la información de los pesos."""
    print(f"  INFO DinamicWeights: Razón Principal = {razon_principal}")
    if detalles_razon:
        for detalle in detalles_razon:
            print(f"    - {detalle}")
    print(f"  INFO DinamicWeights: Pesos Asignados -> BM25={peso_bm25:.2f}, Embedding={peso_emb:.2f}")



print("INFO: Celda 3 - Funciones auxiliares definidas (tokenizer, pesos, normalización).")
print(f"  - Usando tokenizer: {tokenizer_for_bm25.__name__}")
print("--- Fin Celda 3 ---")


def initialize_retriever():
    global app_state
    if app_state.get("is_initialized"):
        return

    print("INFO: Inicializando modelos e índices...")
    try:
        # Descargar tokenizer de NLTK
        nltk.download("punkt", quiet=True)

        # Modelos
        app_state["embeddings_model"] = OpenAIEmbeddings(model=EMBEDDING_MODEL_NAME)
        app_state["reranker"] = CrossEncoder(RERANKER_MODEL)
        app_state["llm"] = ChatGoogleGenerativeAI(model=LLM_MODEL_NAME)

        # Conectar con Pinecone (opcional, no bloquea arranque si falla)
        pinecone_index = None
        try:
            pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
            pinecone_index = pc.Index(os.getenv("PINECONE_INDEX_NAME"))
            app_state["pinecone_index_for_upsert"] = pinecone_index
            print("INFO: Conectado a Pinecone.")
        except Exception as e:
            print(f"ERROR al conectar con Pinecone: {e}")
            app_state["pinecone_index_for_upsert"] = None

        # ⚠️ Evitamos descargar todos los vectores al inicio
        # Cargamos FAISS vacío y lo iremos llenando con upserts
        app_state["faiss_index"] = faiss.IndexFlatL2(DIMENSION)
        app_state["texts"] = []
        app_state["metadatas"] = []
        app_state["bm25"] = BM25Okapi([])

        app_state["is_initialized"] = True
        print("INFO: Sistema listo (FAISS vacío, se llenará con nuevas inserciones).")

    except Exception as e:
        app_state["is_initialized"] = False
        print(f"ERROR en initialize_retriever: {e}")
        traceback.print_exc()


# ==========================
# ENDPOINT: ASK (RAG)
# ==========================
from pydantic import BaseModel

class QueryRequest(BaseModel):
    question: str
    topic: str | None = None

@app.post("/ask")
async def ask_rag_question(request: QueryRequest):
    initialize_retriever()

    if not app_state.get("is_initialized"):
        raise HTTPException(status_code=503, detail="Servicio no inicializado.")

    user_question = request.question
    topic = request.topic

    try:
        # Embedding de la query
        query_embedding = app_state["embeddings_model"].embed_query(user_question)
        query_embedding_np = np.array([query_embedding], dtype=np.float32)

        # FAISS
        distances, faiss_indices = app_state["faiss_index"].search(query_embedding_np, K_FAISS_INITIAL)
        faiss_sims = 1.0 / (1.0 + distances[0])
        faiss_results = {idx: sim for idx, sim in zip(faiss_indices[0], faiss_sims) if idx != -1}

        # BM25
        tokenized_query = simple_tokenizer(user_question)
        all_bm25_scores = app_state["bm25"].get_scores(tokenized_query)
        bm25_top_indices = np.argsort(all_bm25_scores)[::-1][:K_BM25_INITIAL]
        bm25_results = {idx: all_bm25_scores[idx] for idx in bm25_top_indices if all_bm25_scores[idx] > 0}

        # Híbrido
        peso_bm25, peso_emb = calcular_pesos_dinamicos(user_question, topic)
        candidate_ids = set(faiss_results.keys()) | set(bm25_results.keys())

        min_faiss, max_faiss = (min(faiss_results.values()), max(faiss_results.values())) if faiss_results else (0.0, 0.0)
        min_bm25, max_bm25 = (min(bm25_results.values()), max(bm25_results.values())) if bm25_results else (0.0, 0.0)

        hybrid_scores = {
            idx: (peso_emb * norm_score(faiss_results.get(idx, 0.0), min_faiss, max_faiss)) +
                 (peso_bm25 * norm_score(bm25_results.get(idx, 0.0), min_bm25, max_bm25))
            for idx in candidate_ids
        }

        sorted_hybrid_ids = sorted(hybrid_scores, key=hybrid_scores.get, reverse=True)[:K_RERANK]

        # Reranker
        rerank_pairs = [[user_question, app_state["texts"][idx]] for idx in sorted_hybrid_ids]
        if not rerank_pairs:
            return {"answer": "No he encontrado documentos candidatos."}

        reranker_scores = app_state["reranker"].predict(rerank_pairs)
        reranked_docs_info = sorted(
            [
                {
                    "doc_id": doc_id,
                    "text": app_state["texts"][doc_id],
                    "metadata": app_state["metadatas"][doc_id],
                    "reranker_score": float(reranker_scores[i]),
                }
                for i, doc_id in enumerate(sorted_hybrid_ids)
            ],
            key=lambda x: x["reranker_score"],
            reverse=True,
        )

        if USE_DYNAMIC_K:
            selected_for_dynamic_k = [doc for doc in reranked_docs_info if doc["reranker_score"] >= RERANKER_SCORE_THRESHOLD]
            final_top_docs = reranked_docs_info[:MIN_CHUNKS_DYNAMIC] if len(selected_for_dynamic_k) < MIN_CHUNKS_DYNAMIC else selected_for_dynamic_k[:MAX_CHUNKS_DYNAMIC]
        else:
            final_top_docs = reranked_docs_info[:3]

        context = "\n\n---\n\n".join([doc["text"] for doc in final_top_docs])
        if not context:
            return {"answer": "No he encontrado información relevante."}

        # RAG con LangChain
        qa_prompt_template = ChatPromptTemplate.from_messages([
            ("system", "Responde basándote solo en el contexto."),
            ("human", "Contexto:\n{context}\n\nPregunta:\n{question}")
        ])

        rag_chain = (
            {"context": lambda x: context, "question": RunnablePassthrough()}
            | qa_prompt_template
            | app_state["llm"]
            | StrOutputParser()
        )

        final_answer = rag_chain.invoke({"context": context, "question": user_question})

        return {"answer": final_answer}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error en /ask: {e}")

# ==========================
# ENDPOINT: PROCESS PDF
# ==========================
def embed_batch(batch_texts: list[str]) -> list[list[float]]:
    if "embeddings_model" not in app_state:
        initialize_retriever()
    embeddings_model = app_state.get("embeddings_model")
    return embeddings_model.embed_documents(batch_texts)

@app.post("/process-and-save-pdf")
async def process_and_save_pdf(file: UploadFile = File(...)):
    if "pinecone_index_for_upsert" not in app_state:
        initialize_retriever()
    pinecone_index = app_state.get("pinecone_index_for_upsert")
    if not pinecone_index:
        raise HTTPException(status_code=503, detail="Pinecone no disponible.")

    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_pdf:
        temp_pdf_path = temp_pdf.name
        temp_pdf.write(await file.read())

    try:
        extraction_result = extract_and_clean_pdf_smart_stem(pdf_path=temp_pdf_path)
        cleaned_text = extraction_result.get("cleaned_text")
        if not cleaned_text:
            return {"status": "success", "vectors_added": 0}

        chunks = chunk_text_by_sentences(cleaned_text)
        if not chunks:
            return {"status": "success", "vectors_added": 0}

        vectors_to_upsert = []
        for i in range(0, len(chunks), 100):
            batch_chunks = chunks[i: i + 100]
            batch_embeddings = embed_batch(batch_chunks)

            for j, chunk_text in enumerate(batch_chunks):
                vectors_to_upsert.append({
                    "id": str(uuid.uuid4()),
                    "values": batch_embeddings[j],
                    "metadata": {
                        "text": chunk_text,
                        "original_filename": file.filename,
                        "chunk_index": i + j,
                    },
                })

        if vectors_to_upsert:
            pinecone_index.upsert(vectors=vectors_to_upsert)

        return {"status": "success", "vectors_added": len(vectors_to_upsert)}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error en /process-and-save-pdf: {e}")

    finally:
        if os.path.exists(temp_pdf_path):
            os.unlink(temp_pdf_path)
     
